{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "binary-contents",
   "metadata": {},
   "source": [
    "# Support Vector Machine\n",
    "**support-vector machines** are supervised learning models with associated learning [algorithms](https://en.wikipedia.org/wiki/Algorithm) that analyze data used for [classification](https://en.wikipedia.org/wiki/Statistical_classification) and [regression analysis](https://en.wikipedia.org/wiki/Regression_analysis). From SVM can be derived a non-[probabilistic](https://en.wikipedia.org/wiki/Probabilistic_classification) [binary](https://en.wikipedia.org/wiki/Binary_classifier) [linear classifier](https://en.wikipedia.org/wiki/Linear_classifier). An SVM model is a representation of the examples as points in space, mapped so that the examples of the separate categories are divided by a clear gap that is as wide as possible. New examples are then mapped into that same space and predicted to belong to a category based on the side of the gap on which they fall.\n",
    "\n",
    "In addition to performing [linear classification](https://en.wikipedia.org/wiki/Linear_classifier), SVMs can efficiently perform a non-linear classification using what is called the [kernel trick](https://en.wikipedia.org/wiki/Kernel_method#Mathematics:_the_kernel_trick), implicitly mapping their inputs into high-dimensional feature spaces."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fluid-kenya",
   "metadata": {},
   "source": [
    "The main idea behind SVM is to discriminate two classes using a *linear separation* of the points. Considering points in a $p$-dimensional space, we want to find a $(p-1)$-dimensional hyperplane that separates the classes. If the points are linearly separable, there may be many hyperplanes that might classify the data. One reasonable choice as the best hyperplane is the one that provides the largest separation, or *margin*, between the two classes. We choose the hyperplane so that the distance from it to the nearest data point on each side is maximized. If such a hyperplane exists, it is known as the **maximum-margin hyperplane** and the linear classifier it defines is known as a maximum-margin classifier; or equivalently, the perceptron of optimal stability.\n",
    "<img src = \"./images/linear-separation.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "agreed-texas",
   "metadata": {},
   "source": [
    "### Hyperplane\n",
    "What is an hyperplane? An hyperplane in $p$ dimensions is a flat affine subspace of dimension $p-1$, with general equation:\n",
    "\n",
    "$$ \\beta_0 + \\beta_1x_1 + \\beta_2x_2 + \\ldots + \\beta_px_p = 0$$\n",
    "\n",
    "Note some basics properties:\n",
    "- In $p=2$ dimensions an hyperplane is simply a line\n",
    "- If $\\beta_0 = 0$ the hyperplane goes through the origin\n",
    "- The vector $\\mathbb \\beta = \\begin{pmatrix}\\beta_1&\\beta_2&\\ldots &\\beta_p\\end{pmatrix}$ is called the **normal vector**. It points to the direction orthogonal to the surface of the hyperplane"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "damaged-exchange",
   "metadata": {},
   "source": [
    "An hyperplane defines a separation of the original $p$ dimensional space into two subspaces. Given a point $X = \\begin{pmatrix}x_1,&x_2,&\\ldots&x_p\\end{pmatrix}$ and $f(X) = beta_0 + \\beta_1X_1 + \\beta_2X_2 + \\ldots + \\beta_pX_p = 0$, we have that:\n",
    "- $ f(X) > 0 $ for points on one side of the hyperplane\n",
    "- $ f(X) < 0 $ for points on the other side of the hyperplane\n",
    "- $ f(X) = 0 $ for points on the hyperplaneR"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
